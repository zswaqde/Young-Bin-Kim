{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img, save_img\n",
    "\n",
    "# Data Transformation and Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Model Training and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Model Building\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, MaxPooling2D, \\\n",
    "                                    Permute, TimeDistributed, Bidirectional, GRU, SimpleRNN, LSTM, \\\n",
    "                                    GlobalAveragePooling2D, Input, concatenate, multiply\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Excel file\n",
    "excel_path = \"C:\\\\Users\\\\dktjw\\\\OneDrive\\\\바탕 화면\\\\Brain hemorrhage detection model\\\\hemorrhage_diagnosis_final.xlsx\"\n",
    "\n",
    "# Path to the image folder\n",
    "image_base_path = \"C:\\\\Users\\\\dktjw\\\\OneDrive\\\\바탕 화면\\\\Brain hemorrhage detection model\\\\CT_bone_data\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Set label column names\n",
    "labels = {\n",
    "    'Intraventricular(뇌실내출혈)': 'Intraventricular',\n",
    "    'Intraparenchymal(뇌실질내출혈)': 'Intraparenchymal',\n",
    "    'Subarachnoid(지주막하출혈)': 'Subarachnoid',\n",
    "    'Epidural(경막외출혈)': 'Epidural',\n",
    "    'Subdural(경막하출혈)': 'Subdural',\n",
    "    'No_Hemorrhage': 'No hemorrhage'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "brain_data = []\n",
    "bone_data = []\n",
    "\n",
    "# Function to convert to three-digit number\n",
    "def make_folder_name(folder_num):\n",
    "    return str(folder_num).zfill(3)\n",
    "\n",
    "# Labeling image files\n",
    "for index, row in df.iterrows():\n",
    "    folder_name = make_folder_name(row['PatientNumber'])  # Convert folder name to three digits\n",
    "    image_name = str(row['SliceNumber']) + '.jpg'  # Add extension to image name\n",
    "\n",
    "    # Determine label\n",
    "    label = None\n",
    "    for col, label_name in labels.items():\n",
    "        if row[col] == 1:\n",
    "            label = label_name\n",
    "            break\n",
    "    \n",
    "    # Label images in brain and bone folders\n",
    "    for subfolder in ['brain', 'bone']:\n",
    "        image_folder_path = os.path.join(image_base_path, folder_name, subfolder)\n",
    "        image_path = os.path.join(image_folder_path, image_name)\n",
    "\n",
    "        # Save results\n",
    "        if subfolder == 'brain':\n",
    "            brain_data.append((image_path, label))\n",
    "        else:\n",
    "            bone_data.append((image_path, label))\n",
    "\n",
    "# Create DataFrames for Brain_Data and Bone_Data\n",
    "CT_Data = pd.DataFrame(brain_data, columns=['Image', 'Label'])\n",
    "Bone_Data = pd.DataFrame(bone_data, columns=['Image', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CHECKING NUMBER OF DATA LABEL\n",
    "print(CT_Data[\"Label\"].value_counts())\n",
    "print(Bone_Data[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Brain and Bone datasets\n",
    "combined_data = pd.concat([CT_Data, Bone_Data], axis=1)\n",
    "\n",
    "# Split into Train and Test sets\n",
    "Train_Data, Test_Data = train_test_split(combined_data, train_size=0.8, shuffle=True, random_state=42)\n",
    "\n",
    "# Separate Brain data and Bone data in each Train and Test set\n",
    "Train_Data_Brain = Train_Data.iloc[:, [0, 1]]  # Select first and second columns\n",
    "Train_Data_Bone = Train_Data.iloc[:, [2, 3]]   # Select third and fourth columns\n",
    "Test_Data_Brain = Test_Data.iloc[:, [0, 1]]    # Select first and second columns\n",
    "Test_Data_Bone = Test_Data.iloc[:, [2, 3]]     # Select third and fourth columns\n",
    "\n",
    "# Count the number of samples for each label in the Test set\n",
    "brain_test_label_counts = Test_Data_Brain['Label'].value_counts()\n",
    "bone_test_label_counts = Test_Data_Bone['Label'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\\nBRAIN TEST LABEL COUNTS:\")\n",
    "print(brain_test_label_counts)\n",
    "\n",
    "print(\"\\nBONE TEST LABEL COUNTS:\")\n",
    "print(bone_test_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA AUGMENTATION\n",
    "\n",
    "# Data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def augment_data(brain_df, bone_df, output_folder, target_count=2500):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    augmented_brain_data = []\n",
    "    augmented_bone_data = []\n",
    "\n",
    "    for label in brain_df['Label'].unique():\n",
    "        brain_label_df = brain_df[brain_df['Label'] == label]\n",
    "        bone_label_df = bone_df[bone_df['Label'] == label]\n",
    "\n",
    "        current_count = len(brain_label_df)\n",
    "        label_brain_folder = os.path.join(output_folder, f\"{label}_brain\")\n",
    "        label_bone_folder = os.path.join(output_folder, f\"{label}_bone\")\n",
    "        os.makedirs(label_brain_folder, exist_ok=True)\n",
    "        os.makedirs(label_bone_folder, exist_ok=True)\n",
    "\n",
    "        # Sequentially copy existing images to label folders\n",
    "        for idx, (brain_row, bone_row) in enumerate(zip(brain_label_df.itertuples(), bone_label_df.itertuples()), start=1):\n",
    "            brain_img_path = brain_row.Image\n",
    "            bone_img_path = bone_row.Image\n",
    "\n",
    "            dest_brain_path = os.path.join(label_brain_folder, f\"{label}_brain{idx}.jpg\")\n",
    "            dest_bone_path = os.path.join(label_bone_folder, f\"{label}_bone{idx}.jpg\")\n",
    "\n",
    "            shutil.copy(brain_img_path, dest_brain_path)\n",
    "            shutil.copy(bone_img_path, dest_bone_path)\n",
    "\n",
    "            augmented_brain_data.append((dest_brain_path, label))\n",
    "            augmented_bone_data.append((dest_bone_path, label))\n",
    "\n",
    "        # Augment to make up for missing count\n",
    "        augment_count = target_count - current_count\n",
    "        if augment_count > 0:\n",
    "            brain_imgs_to_augment = brain_label_df['Image'].tolist()\n",
    "            bone_imgs_to_augment = bone_label_df['Image'].tolist()\n",
    "            augment_per_img = augment_count // len(brain_imgs_to_augment) + 1\n",
    "\n",
    "            for brain_img_path, bone_img_path in zip(brain_imgs_to_augment, bone_imgs_to_augment):\n",
    "                brain_img = load_img(brain_img_path)\n",
    "                bone_img = load_img(bone_img_path)\n",
    "\n",
    "                x_brain = img_to_array(brain_img)\n",
    "                x_bone = img_to_array(bone_img)\n",
    "\n",
    "                x_brain = x_brain.reshape((1,) + x_brain.shape)\n",
    "                x_bone = x_bone.reshape((1,) + x_bone.shape)\n",
    "\n",
    "                i = 0\n",
    "                for brain_batch, bone_batch in zip(datagen.flow(x_brain, batch_size=1), datagen.flow(x_bone, batch_size=1)):\n",
    "                    if i >= augment_per_img or len(os.listdir(label_brain_folder)) >= target_count:\n",
    "                        break\n",
    "\n",
    "                    new_brain_img = array_to_img(brain_batch[0])\n",
    "                    new_bone_img = array_to_img(bone_batch[0])\n",
    "\n",
    "                    current_index = len(os.listdir(label_brain_folder)) + 1\n",
    "\n",
    "                    new_brain_img_name = f\"{label}_brain{current_index}.jpg\"\n",
    "                    new_bone_img_name = f\"{label}_bone{current_index}.jpg\"\n",
    "\n",
    "                    new_brain_img_path = os.path.join(label_brain_folder, new_brain_img_name)\n",
    "                    new_bone_img_path = os.path.join(label_bone_folder, new_bone_img_name)\n",
    "\n",
    "                    save_img(new_brain_img_path, brain_batch[0])\n",
    "                    save_img(new_bone_img_path, bone_batch[0])\n",
    "\n",
    "                    augmented_brain_data.append((new_brain_img_path, label))\n",
    "                    augmented_bone_data.append((new_bone_img_path, label))\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "    # Create augmented DataFrames\n",
    "    augmented_brain_df = pd.DataFrame(augmented_brain_data, columns=['Image', 'Label'])\n",
    "    augmented_bone_df = pd.DataFrame(augmented_bone_data, columns=['Image', 'Label'])\n",
    "\n",
    "    return augmented_brain_df, augmented_bone_df\n",
    "\n",
    "def find_missing_numbers(nums, target_count=2500):\n",
    "    \"\"\"\n",
    "    Find and return missing numbers in the given list.\n",
    "    \"\"\"\n",
    "    missing_nums = [i for i in range(1, target_count + 1) if i not in nums]\n",
    "    return missing_nums\n",
    "\n",
    "def fill_missing_images(df, output_folder, target_count=2500):\n",
    "    \"\"\"\n",
    "    Find missing numbers in the given DataFrame and augment data to fill those numbers.\n",
    "    \"\"\"\n",
    "    global datagen\n",
    "\n",
    "    augmented_brain_data = []\n",
    "    augmented_bone_data = []\n",
    "\n",
    "    for label in df['Label'].unique():\n",
    "        label_folder = os.path.join(output_folder, f\"{label}_brain\")\n",
    "        existing_files = [int(f.split(label + \"_brain\")[1].split(\".jpg\")[0]) for f in os.listdir(label_folder)]\n",
    "        missing_nums = find_missing_numbers(existing_files, target_count)\n",
    "\n",
    "        if not missing_nums:\n",
    "            # Already have 2500 images\n",
    "            continue\n",
    "\n",
    "        for num in missing_nums:\n",
    "            brain_img_path = df[df['Label'] == label]['Image'].sample().values[0]\n",
    "            bone_img_path = brain_img_path.replace('brain', 'bone')\n",
    "\n",
    "            brain_img = load_img(brain_img_path)\n",
    "            bone_img = load_img(bone_img_path)\n",
    "\n",
    "            x_brain = img_to_array(brain_img)\n",
    "            x_bone = img_to_array(bone_img)\n",
    "\n",
    "            x_brain = x_brain.reshape((1,) + x_brain.shape)\n",
    "            x_bone = x_bone.reshape((1,) + x_bone.shape)\n",
    "\n",
    "            brain_batch = next(datagen.flow(x_brain, batch_size=1))\n",
    "            bone_batch = next(datagen.flow(x_bone, batch_size=1))\n",
    "\n",
    "            new_brain_img = array_to_img(brain_batch[0])\n",
    "            new_bone_img = array_to_img(bone_batch[0])\n",
    "\n",
    "            new_brain_img_name = f\"{label}_brain{num}.jpg\"\n",
    "            new_bone_img_name = f\"{label}_bone{num}.jpg\"\n",
    "\n",
    "            new_brain_img_path = os.path.join(label_folder, new_brain_img_name)\n",
    "            new_bone_img_path = os.path.join(label_folder.replace('brain', 'bone'), new_bone_img_name)\n",
    "\n",
    "            save_img(new_brain_img_path, brain_batch[0])\n",
    "            save_img(new_bone_img_path, bone_batch[0])\n",
    "\n",
    "            augmented_brain_data.append((new_brain_img_path, label))\n",
    "            augmented_bone_data.append((new_bone_img_path, label))\n",
    "\n",
    "    # Add newly created images to the existing DataFrame\n",
    "    augmented_brain_df = pd.concat([df, pd.DataFrame(augmented_brain_data, columns=['Image', 'Label'])])\n",
    "    augmented_bone_df = pd.concat([df, pd.DataFrame(augmented_bone_data, columns=['Image', 'Label'])])\n",
    "\n",
    "    return augmented_brain_df, augmented_bone_df\n",
    "\n",
    "# Example DataFrames creation (replace with actual data)\n",
    "# Train_Data_Brain = pd.DataFrame([...])  # Actual Train Brain DataFrame\n",
    "# Train_Data_Bone = pd.DataFrame([...])  # Actual Train Bone DataFrame\n",
    "\n",
    "# Set folder to save augmented data\n",
    "output_folder = 'Augmented_Data'\n",
    "\n",
    "# Perform data augmentation\n",
    "Augmented_Train_Data_Brain, Augmented_Train_Data_Bone = augment_data(Train_Data_Brain, Train_Data_Bone, output_folder)\n",
    "\n",
    "# Fill missing images if any and augment\n",
    "filled_Augmented_Train_Data_Brain, filled_Augmented_Train_Data_Bone = fill_missing_images(Augmented_Train_Data_Brain, output_folder)\n",
    "filled_Augmented_Train_Data_Brain, filled_Augmented_Train_Data_Bone = fill_missing_images(Augmented_Train_Data_Bone, output_folder)\n",
    "\n",
    "# Print final results\n",
    "print(\"Augmented Train Data Brain:\")\n",
    "print(filled_Augmented_Train_Data_Brain.head())\n",
    "\n",
    "print(\"\\nAugmented Train Data Bone:\")\n",
    "print(filled_Augmented_Train_Data_Bone.head())\n",
    "\n",
    "\n",
    "## Code to reload the generated image files\n",
    "\n",
    "def load_augmented_images(output_folder):\n",
    "    brain_data = []\n",
    "    bone_data = []\n",
    "\n",
    "    for label_folder in os.listdir(output_folder):\n",
    "        if label_folder.endswith('_brain'):\n",
    "            label = label_folder.split('_brain')[0]\n",
    "            brain_folder_path = os.path.join(output_folder, label_folder)\n",
    "            bone_folder_path = brain_folder_path.replace('_brain', '_bone')\n",
    "            \n",
    "            for brain_img_file in os.listdir(brain_folder_path):\n",
    "                brain_img_path = os.path.join(brain_folder_path, brain_img_file)\n",
    "                bone_img_path = os.path.join(bone_folder_path, brain_img_file.replace('_brain', '_bone'))\n",
    "                \n",
    "                brain_data.append((brain_img_path, label))\n",
    "                bone_data.append((bone_img_path, label))\n",
    "\n",
    "    brain_df = pd.DataFrame(brain_data, columns=['Image', 'Label'])\n",
    "    bone_df = pd.DataFrame(bone_data, columns=['Image', 'Label'])\n",
    "\n",
    "    return brain_df, bone_df\n",
    "\n",
    "# Folder where augmented data is saved\n",
    "output_folder = 'Augmented_Data'\n",
    "\n",
    "# Load images from the folder and create DataFrame\n",
    "Augmented_Train_Data_Brain, Augmented_Train_Data_Bone = load_augmented_images(output_folder)\n",
    "\n",
    "# Print final results\n",
    "print(\"Augmented Train Data Brain:\")\n",
    "print(Augmented_Train_Data_Brain.head())\n",
    "\n",
    "print(\"\\nAugmented Train Data Bone:\")\n",
    "print(Augmented_Train_Data_Bone.head())\n",
    "\n",
    "# Check the number of images\n",
    "print(\"\\nTotal number of brain images:\", len(Augmented_Train_Data_Brain))\n",
    "print(\"Total number of bone images:\", len(Augmented_Train_Data_Bone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size):\n",
    "    try:\n",
    "        img = load_img(image_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to load images from dataframe\n",
    "def load_images_from_dataframe(df, target_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        image_path = row['Image']\n",
    "        image = preprocess_image(image_path, target_size)\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            labels.append(row['Label'])\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "# Load images and labels for brain train dataset\n",
    "brain_train_images, brain_train_labels = load_images_from_dataframe(Agumented_Train_Data_Brain, IMG_SIZE)\n",
    "\n",
    "# Load images and labels for brain test dataset\n",
    "brain_test_images, brain_test_labels = load_images_from_dataframe(Test_Data_Brain, IMG_SIZE)\n",
    "\n",
    "# Load images and labels for bone train dataset\n",
    "bone_train_images, bone_train_labels = load_images_from_dataframe(Agumented_Train_Data_Bone, IMG_SIZE)\n",
    "\n",
    "# Load images and labels for bone test dataset\n",
    "bone_test_images, bone_test_labels = load_images_from_dataframe(Test_Data_Bone, IMG_SIZE)\n",
    "\n",
    "# Normalize images\n",
    "brain_train_images = brain_train_images / 255.0\n",
    "brain_test_images = brain_test_images / 255.0\n",
    "bone_train_images = bone_train_images / 255.0\n",
    "bone_test_images = bone_test_images / 255.0\n",
    "\n",
    "# Label encoding and one-hot encoding\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(labels)\n",
    "    one_hot_encoded = to_categorical(integer_encoded)\n",
    "    return one_hot_encoded\n",
    "\n",
    "brain_train_labels_encoded = encode_labels(brain_train_labels)\n",
    "brain_test_labels_encoded = encode_labels(brain_test_labels)\n",
    "bone_train_labels_encoded = encode_labels(bone_train_labels)\n",
    "bone_test_labels_encoded = encode_labels(bone_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enhanced CNN model for brain images\n",
    "input_brain = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_brain)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling2D((2, 2))(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "\n",
    "x1 = Conv2D(64, (3, 3), activation='relu', padding='same')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling2D((2, 2))(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "\n",
    "x1 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling2D((2, 2))(x1)\n",
    "x1 = Dropout(0.4)(x1)\n",
    "\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "# Build the enhanced CNN model for bone images\n",
    "input_bone = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x2 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_bone)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D((2, 2))(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "\n",
    "x2 = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D((2, 2))(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "\n",
    "x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = MaxPooling2D((2, 2))(x2)\n",
    "x2 = Dropout(0.4)(x2)\n",
    "\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "# Classification layer for brain-only model\n",
    "output_brain_only = Dense(6, activation='softmax')(x1)\n",
    "\n",
    "# Combine the outputs using concatenate\n",
    "combined_concat = concatenate([x1, x2])\n",
    "\n",
    "# Product fusion\n",
    "combined_product = multiply([x1, x2])\n",
    "\n",
    "# Classification layer for concatenate fusion\n",
    "combined_concat = Dense(64, activation='relu')(combined_concat)\n",
    "combined_concat = Dropout(0.5)(combined_concat)\n",
    "output_concat = Dense(6, activation='softmax')(combined_concat)\n",
    "\n",
    "# Classification layer for product fusion\n",
    "combined_product = Dense(64, activation='relu')(combined_product)\n",
    "combined_product = Dropout(0.5)(combined_product)\n",
    "output_product = Dense(6, activation='softmax')(combined_product)\n",
    "\n",
    "\n",
    "# Define models\n",
    "model_concat = Model(inputs=[input_brain, input_bone], outputs=output_concat)\n",
    "model_product = Model(inputs=[input_brain, input_bone], outputs=output_product)\n",
    "model_brain_only = Model(inputs=input_brain, outputs=output_brain_only)\n",
    "\n",
    "\n",
    "# Compile models\n",
    "model_concat.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_product.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_brain_only.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "# model_concat.summary()\n",
    "# model_product.summary()\n",
    "# model_brain_only.summar()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint_concat = ModelCheckpoint('best_model_concat.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "model_checkpoint_product = ModelCheckpoint('best_model_product.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "model_checkpoint_brain_only = ModelCheckpoint('best_model_brain_only.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# Train the models\n",
    "history_concat = model_concat.fit([brain_train_images, bone_train_images], brain_train_labels_encoded, \n",
    "                                  epochs=50, \n",
    "                                  batch_size=32, \n",
    "                                  validation_data=([brain_test_images, bone_test_images], brain_test_labels_encoded), \n",
    "                                  callbacks=[early_stopping, model_checkpoint_concat])\n",
    "\n",
    "history_product = model_product.fit([brain_train_images, bone_train_images], brain_train_labels_encoded, \n",
    "                                    epochs=50, \n",
    "                                    batch_size=32, \n",
    "                                    validation_data=([brain_test_images, bone_test_images], brain_test_labels_encoded), \n",
    "                                    callbacks=[early_stopping, model_checkpoint_product])\n",
    "\n",
    "history_brain_only = model_brain_only.fit(brain_train_images, brain_train_labels_encoded,\n",
    "                                          epochs=50,\n",
    "                                          batch_size=32,\n",
    "                                          validation_data=(brain_test_images, brain_test_labels_encoded),\n",
    "                                          callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values for all three models\n",
    "plt.plot(history_brain_only.history['accuracy'])\n",
    "plt.plot(history_brain_only.history['val_accuracy'])\n",
    "plt.plot(history_concat.history['accuracy'])\n",
    "plt.plot(history_concat.history['val_accuracy'])\n",
    "plt.plot(history_product.history['accuracy'])\n",
    "plt.plot(history_product.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Brain Only Train', 'Brain Only Val', 'Concat Train', 'Concat Val', 'Product Train', 'Product Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values for all three models\n",
    "plt.plot(history_brain_only.history['loss'])\n",
    "plt.plot(history_brain_only.history['val_loss'])\n",
    "plt.plot(history_concat.history['loss'])\n",
    "plt.plot(history_concat.history['val_loss'])\n",
    "plt.plot(history_product.history['loss'])\n",
    "plt.plot(history_product.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Brain Only Train', 'Brain Only Val', 'Concat Train', 'Concat Val', 'Product Train', 'Product Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print the best accuracy of each model\n",
    "best_val_accuracy_brain_only = max(history_brain_only.history['accuracy'])\n",
    "best_val_accuracy_concat = max(history_concat.history['accuracy'])\n",
    "best_val_accuracy_product = max(history_product.history['accuracy'])\n",
    "\n",
    "print(f'Best Validation Accuracy for Brain Only Model: {best_val_accuracy_brain_only:.4f}')\n",
    "print(f'Best Validation Accuracy for Concatenate Model: {best_val_accuracy_concat:.4f}')\n",
    "print(f'Best Validation Accuracy for Product Model: {best_val_accuracy_product:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_string(labels):\n",
    "    return np.argmax(labels, axis=1)\n",
    "\n",
    "def evaluate_and_compare_model(model_path, brain_test_images, bone_test_images, brain_test_labels_encoded, model_type='combined'):\n",
    "    best_model = load_model(model_path)\n",
    "    \n",
    "    if model_type == 'brain_only':\n",
    "        test_loss, test_accuracy = best_model.evaluate(brain_test_images, brain_test_labels_encoded)\n",
    "        predicted_labels = best_model.predict(brain_test_images)\n",
    "    else:\n",
    "        test_loss, test_accuracy = best_model.evaluate([brain_test_images, bone_test_images], brain_test_labels_encoded)\n",
    "        predicted_labels = best_model.predict([brain_test_images, bone_test_images])\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy}, Test Loss: {test_loss}\")\n",
    "    \n",
    "    predicted_labels_binary = (predicted_labels > 0.5).astype(int)\n",
    "    \n",
    "    brain_test_labels_decoded = labels_to_string(brain_test_labels_encoded)\n",
    "    predicted_labels_decoded = labels_to_string(predicted_labels_binary)\n",
    "    \n",
    "    test_accuracy = accuracy_score(brain_test_labels_decoded, predicted_labels_decoded)\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Actual': brain_test_labels_decoded,\n",
    "        'Predicted': predicted_labels_decoded\n",
    "    })\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "comparison_brain_only = evaluate_and_compare_model('best_model_brain_only.keras', brain_test_images, None, brain_test_labels_encoded, model_type='brain_only')\n",
    "comparison_concat = evaluate_and_compare_model('best_model_concat.keras', brain_test_images, bone_test_images, brain_test_labels_encoded, model_type='combined')\n",
    "comparison_product = evaluate_and_compare_model('best_model_product.keras', brain_test_images, bone_test_images, brain_test_labels_encoded, model_type='combined')\n",
    "\n",
    "# Save the comparisons to CSV and Excel files\n",
    "comparison_brain_only.to_excel('prediction_comparison_brain_only.xlsx', index=False)\n",
    "comparison_concat.to_excel('prediction_comparison_concat.xlsx', index=False)\n",
    "comparison_product.to_excel('prediction_comparison_product.xlsx', index=False)\n",
    "\n",
    "print(\"Comparison for Brain Only model:\")\n",
    "print(comparison_brain_only.head())\n",
    "\n",
    "print(\"Comparison for Concat model:\")\n",
    "print(comparison_concat.head())\n",
    "\n",
    "print(\"Comparison for Product model:\")\n",
    "print(comparison_product.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['Epidural', 'Intraparenchymal', 'Intraventricular', 'No hemorrhage', 'Subarachnoid', 'Subdural']\n",
    "\n",
    "# Create confusion matrices by comparing predicted and actual values\n",
    "conf_matrix_brain_only = confusion_matrix(labels_to_string(brain_test_labels_encoded), labels_to_string(model_brain_only.predict(brain_test_images)))\n",
    "conf_matrix_concat = confusion_matrix(labels_to_string(brain_test_labels_encoded), labels_to_string(model_concat.predict([brain_test_images, bone_test_images])))\n",
    "conf_matrix_product = confusion_matrix(labels_to_string(brain_test_labels_encoded), labels_to_string(model_product.predict([brain_test_images, bone_test_images])))\n",
    "\n",
    "# Visualize with heatmap\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(conf_matrix_brain_only, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix for Brain Only Model')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(conf_matrix_concat, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix for Concat Model')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(conf_matrix_product, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix for Product Model')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XAI(Gradient-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained models\n",
    "model_brain_only = load_model('best_model_brain_only.keras')\n",
    "model_concat = load_model('best_model_concat.keras')\n",
    "model_product = load_model('best_model_product.keras')\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size):\n",
    "    try:\n",
    "        img = load_img(image_path, target_size=target_size)\n",
    "        img_array = img_to_array(img)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get Grad-CAM heatmap\n",
    "def get_grad_cam(model, img_array_brain, img_array_bone, category_index, layer_name, model_type='combined'):\n",
    "    if model_type == 'brain_only':\n",
    "        grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(img_array_brain)\n",
    "            loss = predictions[:, category_index]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "    else:\n",
    "        grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model([img_array_brain, img_array_bone])\n",
    "            loss = predictions[:, category_index]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        \n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    conv_outputs = conv_outputs.numpy()\n",
    "    \n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        conv_outputs[:, :, i] *= pooled_grads[i]\n",
    "        \n",
    "    heatmap = np.mean(conv_outputs, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    if np.max(heatmap) != 0:\n",
    "        heatmap /= np.max(heatmap)\n",
    "    heatmap = cv2.resize(heatmap, (img_array_brain.shape[2], img_array_brain.shape[1]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    return heatmap\n",
    "\n",
    "# Example visualization for each category\n",
    "categories = ['no hemorrhage', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "brain_layer_name = 'conv2d_1'  # Layer name for brain images in brain_only model\n",
    "combined_brain_layer_name = 'conv2d_1'  # Layer name for brain images in combined models\n",
    "combined_bone_layer_name = 'conv2d_3'   # Layer name for bone images in combined models\n",
    "\n",
    "# Use one of the paths from your test set\n",
    "img_path = Test_Data_Brain.iloc[0]['Image']  # Path to a test brain image\n",
    "bone_img_path = Test_Data_Bone.iloc[0]['Image']  # Path to a test bone image\n",
    "\n",
    "# Preprocess the images\n",
    "brain_img = preprocess_image(img_path, IMG_SIZE)\n",
    "bone_img = preprocess_image(bone_img_path, IMG_SIZE)\n",
    "\n",
    "if brain_img is not None and bone_img is not None:\n",
    "    brain_img_array = np.expand_dims(brain_img, axis=0) / 255.0\n",
    "    bone_img_array = np.expand_dims(bone_img, axis=0) / 255.0\n",
    "\n",
    "    plt.figure(figsize=(20, 40))\n",
    "    for i, category in enumerate(categories):\n",
    "        # Get Grad-CAM heatmap for brain image for each category (Brain Only Model)\n",
    "        brain_heatmap_brain_only = get_grad_cam(model_brain_only, brain_img_array, None, category_index=i, layer_name=brain_layer_name, model_type='brain_only')\n",
    "        \n",
    "        # Get Grad-CAM heatmap for brain image for each category (Concat Model)\n",
    "        brain_heatmap_concat = get_grad_cam(model_concat, brain_img_array, bone_img_array, category_index=i, layer_name=combined_brain_layer_name)\n",
    "\n",
    "        # Get Grad-CAM heatmap for bone image for each category (Concat Model)\n",
    "        bone_heatmap_concat = get_grad_cam(model_concat, brain_img_array, bone_img_array, category_index=i, layer_name=combined_bone_layer_name)\n",
    "        \n",
    "        # Get Grad-CAM heatmap for brain image for each category (Product Model)\n",
    "        brain_heatmap_product = get_grad_cam(model_product, brain_img_array, bone_img_array, category_index=i, layer_name=combined_brain_layer_name)\n",
    "\n",
    "        # Get Grad-CAM heatmap for bone image for each category (Product Model)\n",
    "        bone_heatmap_product = get_grad_cam(model_product, brain_img_array, bone_img_array, category_index=i, layer_name=combined_bone_layer_name)\n",
    "        \n",
    "        # Display the Grad-CAM heatmap overlayed on the original image (Brain Only Model)\n",
    "        plt.subplot(len(categories), 4, 4 * i + 1)\n",
    "        plt.imshow(brain_img / 255.0)\n",
    "        plt.imshow(brain_heatmap_brain_only, cmap='jet', alpha=0.5)\n",
    "        plt.title(f'Grad-CAM Brain Only: Brain Image - {category}')\n",
    "\n",
    "        # Display the Grad-CAM heatmap overlayed on the original image (Concat Model)\n",
    "        plt.subplot(len(categories), 4, 4 * i + 2)\n",
    "        plt.imshow(brain_img / 255.0)\n",
    "        plt.imshow(brain_heatmap_concat, cmap='jet', alpha=0.5)\n",
    "        plt.title(f'Grad-CAM Concat: Brain Image - {category}')\n",
    "\n",
    "        plt.subplot(len(categories), 4, 4 * i + 3)\n",
    "        plt.imshow(bone_img / 255.0)\n",
    "        plt.imshow(bone_heatmap_concat, cmap='jet', alpha=0.5)\n",
    "        plt.title(f'Grad-CAM Concat: Bone Image - {category}')\n",
    "        \n",
    "        # Display the Grad-CAM heatmap overlayed on the original image (Product Model)\n",
    "        plt.subplot(len(categories), 4, 4 * i + 4)\n",
    "        plt.imshow(brain_img / 255.0)\n",
    "        plt.imshow(brain_heatmap_product, cmap='jet', alpha=0.5)\n",
    "        plt.title(f'Grad-CAM Product: Brain Image - {category}')\n",
    "\n",
    "        plt.subplot(len(categories), 4, 4 * i + 5)\n",
    "        plt.imshow(bone_img / 255.0)\n",
    "        plt.imshow(bone_heatmap_product, cmap='jet', alpha=0.5)\n",
    "        plt.title(f'Grad-CAM Product: Bone Image - {category}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: One or both images could not be loaded. Please check the image paths.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
